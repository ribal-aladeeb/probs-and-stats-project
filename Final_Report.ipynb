{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability and Statistics Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, our team decided to investigate the academic performance of students in universities. We realized that our hypthesis was not well defined thus we changed it.\n",
    "Write text balbabaa\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "Below is our code necessary to plot and compute the parameters for this study.\n",
    "<br>You can skip to our *Hypothesis Testing section*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import math\n",
    "import csv\n",
    "import scipy.stats as stats\n",
    "\n",
    "Z_from_probability = stats.norm.ppf\n",
    "probability_of_Z = stats.norm.cdf\n",
    "\n",
    "# list respondents who gave fake answers or made mistakes in answering.\n",
    "# They have to be removed from the analysis in order not to skew the results\n",
    "blacklisted_emails = [\n",
    "    'CleanYourData@Hal3anneh.com',\n",
    "    'nope@hotmail.com',\n",
    "    'eyePISSEDandFARDEDandSHIDDEDandCAMEallOVERthePLACE@lmao.pwned.com',\n",
    "    'chadi.sargi@gmail.com'\n",
    "]\n",
    "\n",
    "original_survey = \"./data/condensed.actual.latest.csv\"\n",
    "numerical_survey = './data/condensed.numerical.latest.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Data\n",
    "This function cleans the data from the imported csv file. We used survey monkey to collect our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(survey_filename: str):\n",
    "    '''\n",
    "    Remove the data points where people purpusefully tried to lie.\n",
    "    Remove all metadata unrelated to business logic\n",
    "    '''\n",
    "    file = survey_filename\n",
    "    numerical_raw_survey = pd.read_csv(file)\n",
    "    total_rows = len(numerical_raw_survey.index)\n",
    "\n",
    "    #print(total_rows)\n",
    "\n",
    "    indexes_to_be_removed = []\n",
    "\n",
    "    for i in range(1, total_rows):\n",
    "        email_of_respondent = numerical_raw_survey.iloc[i, 17]\n",
    "        gpa_of_respondent = float(numerical_raw_survey.iloc[i, 16])\n",
    "        if email_of_respondent in blacklisted_emails or gpa_of_respondent <= 0.1:\n",
    "            indexes_to_be_removed.append(i)\n",
    "\n",
    "    numerical_raw_survey = numerical_raw_survey.drop(\n",
    "        axis=0, index=indexes_to_be_removed)\n",
    "    #print(len(numerical_raw_survey.index))\n",
    "\n",
    "    # only keep answers to survey questions, exclude the faculty question\n",
    "    # 17 is the index of the email column\n",
    "    responses = numerical_raw_survey.iloc[1:, list(range(11, 17))]\n",
    "\n",
    "    # cast string entries to float\n",
    "    responses = pd.DataFrame.astype(responses, float)\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics Functions\n",
    "The functions below create the necessary objects to visualize the data.\n",
    "<br>\n",
    "Moreover, they are used to compute, the sample mean, variance standard deviation, standard error and the Z & p-values & Beta error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned_dataframe(df):\n",
    "    df.to_csv('./data/analysis.ready.data.csv', index=None, header=True)\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_gpas_alone(data):\n",
    "    questions = data.columns\n",
    "    plt.plot(data[questions[5]], 'ro')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def sample_size(data):\n",
    "    return len(data.index)\n",
    "\n",
    "\n",
    "def gpa_list(data):\n",
    "    return list(data.iloc[:, 5])\n",
    "\n",
    "\n",
    "def mean_gpa(data):\n",
    "    gpas = gpa_list(data)\n",
    "    sample_mean = sum(gpas)/len(gpas)\n",
    "    return sample_mean\n",
    "\n",
    "\n",
    "def sample_gpa_variance(data):\n",
    "    # Extract gpas as numpy array to propagate math operations without looping\n",
    "    # over each element.\n",
    "    gpas = np.array(gpa_list(data))\n",
    "    variance = (sum(gpas**2) - sum(gpas)**2/len(gpas)) / (len(gpas)-1)\n",
    "    return variance\n",
    "\n",
    "\n",
    "def std_dev_gpa(data):\n",
    "    return math.sqrt(sample_gpa_variance(data))\n",
    "\n",
    "\n",
    "def median_gpa(data):\n",
    "    gpas = gpa_list(data)\n",
    "    midpoint = int(len(gpas)/2)\n",
    "    if len(gpas) % 2 == 0:\n",
    "        return gpas[midpoint]\n",
    "    lower = gpas[midpoint]\n",
    "    upper = gpas[midpoint+1]\n",
    "    median = (lower+upper)/2\n",
    "    return median\n",
    "\n",
    "\n",
    "def calculate_Z_value(mu0, X_bar, S, n):\n",
    "    # because our sample size is greater than 200, we feel confident of\n",
    "    # estimating sigma using S. In this way we can use the Z-distribution\n",
    "    # instead of the T-distribution. This is often done when n > 30.\n",
    "    Z = (X_bar - mu0)/(S/math.sqrt(n))\n",
    "    return Z\n",
    "\n",
    "\n",
    "def calculate_p_value(data, mu0=3.2):\n",
    "    X_bar = mean_gpa(data)\n",
    "    S = std_dev_gpa(data)\n",
    "    n = sample_size(data)\n",
    "    Z = abs(calculate_Z_value(mu0, X_bar, S, n))\n",
    "    p_value = 2*probability_of_Z(-Z)\n",
    "    return p_value, Z\n",
    "\n",
    "\n",
    "def find_Z_alpha(confidence):\n",
    "    alpha = 1-confidence\n",
    "    Z_alpha_over2 = abs(Z_from_probability(1-alpha/2))\n",
    "    return alpha, Z_alpha_over2\n",
    "\n",
    "\n",
    "def beta_error(Zalpha2, sigma, n, mu0=3.2, mu1=3.3):\n",
    "    old_Z = abs(Zalpha2)\n",
    "    diff = mu1-mu0\n",
    "    beta = probability_of_Z(old_Z-(diff*math.sqrt(n)/sigma))\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Test\n",
    "The functions below uses the following procedure to test sample data.\n",
    "<br> \n",
    "\\begin{align}\n",
    "y = \\frac{(j - 0.5)}{n} \\\\\n",
    "X = x_{i}\n",
    "\\end{align}\n",
    "Where $y$ represents the probabilites, $X$ data points and $j$ the integers from 1 to n.\n",
    "<br>\n",
    "Moreover, it will try to fit a best line to the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_plot(data):\n",
    "    gpas = gpa_list(data)\n",
    "    print(type(\"gpa type:\",gpas))\n",
    "    js = [j+1 for j in range(0, len(gpas))]\n",
    "    prob_dis = [(j-0.5) / (len(js)) for j in js]\n",
    "    plt.scatter(gpas, prob_dis, s=7, alpha=0.5)\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "\n",
    "    b, m = polyfit(gpas, prob_dis, 1)\n",
    "    plt.plot(gpas, prob_dis)\n",
    "    plt.plot(gpas, b + m * gpas, '-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr = pd.read_csv(\"data/Success Factors Study.csv\")\n",
    "sr.drop(sr.index[0], inplace=True)\n",
    "gpas = sr['Finally, what is your gpa?']\n",
    "gpas = gpas.sort_values()\n",
    "gpas = gpas.drop_duplicates()\n",
    "gpas = gpas.iloc[2:]\n",
    "type(gpas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the data and computing the statistics ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = clean(numerical_survey)\n",
    "n = sample_size(cleaned)\n",
    "xbar = mean_gpa(cleaned)\n",
    "S_squared = sample_gpa_variance(cleaned)\n",
    "S = std_dev_gpa(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bar{x}$, $S^{2}$, $S$ and median are show below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size: 206\n",
      "mean: 3.289330097087381\n",
      "variance: 0.2687755002604673\n",
      "standard dev: 0.5184356278849548\n",
      "median: 3.4\n"
     ]
    }
   ],
   "source": [
    "print(\"sample size:\", n)\n",
    "print(\"mean:\", xbar)\n",
    "print(\"variance:\", S_squared)\n",
    "print(\"standard dev:\", S)\n",
    "print(\"median:\", median_gpa(cleaned))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "### Step 1-3\n",
    "For this study, the $\\mu$ of the gpas collected is our parameter of interest and we settled on two sides hypothetis.\n",
    "<br> <br>\n",
    "$H_{0}$: $\\mu = 3.2$ \n",
    "<br>\n",
    "$H_{1}$: $\\mu \\neq 3.2$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Testing Our Data\n",
    "Before proceding to step 4 of our testing, lets perform the normal test mentioned previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type() takes 1 or 3 arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-23faef79eddc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnormal_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-ac235523f2d2>\u001b[0m in \u001b[0;36mnormal_plot\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mnormal_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mgpas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpa_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gpa type:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgpas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprob_dis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: type() takes 1 or 3 arguments"
     ]
    }
   ],
   "source": [
    "js = [j+1 for j in range(0, len(gpas))]\n",
    "prob_dis = [(j-0.5) / (len(js)) for j in js]\n",
    "prob_dis\n",
    "plt.t(gpas, prob_dis)\n",
    "\n",
    "#plt.scatter(gpas, prob_dis, s=7, alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, blabalba normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 \n",
    "Our sample size is $n = 211$.\n",
    "<br>\n",
    "Therefore we can use the Z-distribution since the sample size is large enough. \n",
    "<br>\n",
    "$Z_{ \\alpha / 2}= \\frac{\\mu - \\bar{x}}{s / \\sqrt{n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "Our sample size is large enough, thus we decided to use $\\alpha = 0.02$ for our confidence level.\n",
    "<br>\n",
    "Our resulting critical values $\\pm Z_{ \\alpha / 2} $ are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.020000000000000018\n",
      "Z alpha/2: 2.3263478740408408\n"
     ]
    }
   ],
   "source": [
    "alpha, cutoff_Z = find_Z_alpha(0.98)\n",
    "print(\"alpha:\", alpha)\n",
    "print(\"Z alpha/2:\", cutoff_Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, if our test statistic $Z_{0} < -2.33$ or $Z_{0} > 2.33$, we will reject our null hypthesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "Now, we compute our test statistic $Z_{0}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual Z: 2.473070954112705\n"
     ]
    }
   ],
   "source": [
    "mu0=3.2\n",
    "p_value, actual_Z = calculate_p_value(cleaned, mu0=mu0)\n",
    "print(\"actual Z:\", actual_Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "Lets compute the p-value before drawing our conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_value: 0.013395754395141526\n"
     ]
    }
   ],
   "source": [
    "print(\"p_value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p < \\alpha$, thus we have enough evidence to our reject our null hypothesis <br>\n",
    "$H_{0}$: $\\mu_{0} = 3.2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing Type I & II Error\n",
    "We saw that our type I error was $0.02$ or $2 \\%$ of rejecting our $H_{0}$ when it is true. \n",
    "<br>\n",
    "Now, we will compute our type II error assuming the true $\\mu = 3.3$.\n",
    "<br>\n",
    "Following the formula for $\\beta$\n",
    "\\begin{align}\n",
    "\\beta = \\phi[Z_{ \\alpha / 2} - \\frac{\\delta\\sqrt(n)}{s}] - \\phi[-Z_{ \\alpha / 2} - \\frac{\\delta\\sqrt(n)}{s}]\n",
    "\\end{align}\n",
    "Gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta error: 0.32920284095428376\n",
      "power of test: 0.6707971590457162\n"
     ]
    }
   ],
   "source": [
    "beta = beta_error(cutoff_Z, S, n)\n",
    "print(\"beta error:\", beta)\n",
    "print(\"power of test:\", 1-beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have $33\\%$ of accepting our $H_{0}$ when it is false. \n",
    "<br>\n",
    "Thus, we do not have enough evidence to reject $H_{1}: \\mu \\neq 3.2$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
